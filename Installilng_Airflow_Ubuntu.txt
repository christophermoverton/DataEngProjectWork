Python virtual environment Airflow install on Ubuntu machine:
1.  Ensure that you have pip or pip3 installed on your machine. (e.g., Sudo apt-get install pip3)
2.  Install virtualenv.  At terminal:  Sudo apt-get install virtualenv.  
Note: some guides suggest creating airflow_workspace directory and then sub-directory of airflow and installing
airflow this way.  Apache airflow installs to the sub-directory with an export command assignment 
of the AIRFLOW_HOME path, but after running the webserver and scheduler, I've found unless you establish from 
the outset this unique path assignment relative defaults, then you'll have to go back to pip management 
and properly install as needed, so that the virutal environment is reading from say the correct directory assignment
from configuration, dag folder and log folder.  Otherwise, I've had the virtual environment defaulting to ~/airflow
for installation and not the sub-directory for AIRFLOW_HOME path
3.  At terminal: mkdir airflow 
4.  At terminal: cd airflow
5.  At terminal: virtualenv airflow_env
6.  At terminal: source airflow_env/bin/activate
7.  You should see (airflow_env) your_terminal_handle$  Indicating virtual environment activated.  Now you can install local packages 
for this environment.  These modules are exclusive to this environment only and are not globally installed, so to use these 
packages/modules and subsequent commands related, once installed in the virtual environment, you need to reactivate 
the virtual environment (if it isn't active), when you want to run these packages.  Step 5-6 will allow you 
to reactivate the virtual environment as needed once it is created
8.  at terminal:  pip3 install apache-airflow  
9.  at terminal: pip3 install pandas 
10. at terminal: airflow db init
11. at terminal: airflow users create --username your_admin_name --password your_admin_password --firstname your_first_name --lastname your_last_name --role Admin --email your_email@email.com

you can verify at terminal user name:
airflow users list

12.  at terminal: airflow webserver
Open another terminal tab, and launch the virtual environment per above.

13.  at terminal: airflow scheduler
Open another terminal tab, and launch the virtual environment per above.
We can use this tab to verify loaded dags and do other terminal related stuff.
14.  follow prompts to go to the correct localhost and port address as given on terminal webserver tab. 
That is copy html address link and paste in your web browser.
15.  Use the login and password from step 11 for sign in
If all is well, you should see the Airflow home page with example DAGs listed at the Dashboard.
16.  You can follow template example of any example or the script I've furnished in creating a DAG file:
though you will need to have the proper library for DAG imported in script and DAG defined in script
for Airflow to interpret properly your .py script as a DAG file.  
17. at terminal in the ~/airflow directory: mkdir dags
at terminal: ls 
should show now dags and logs folder alongsider your airflow.cfg file.
18. you can transfer your_properly_defined_DAG_scripts.py to this dags folder
19.  at terminal: airflow dags list
will confirm loaded dags.  It may take a little bit for the DAG to be read and load when refreshing the airflow
dashboard page.

Additionally I've had an xcom related error in script owing to a missing module 'pyarrow' from the script
example,I've furnished.  Perhaps, if you may be able to replicate this error and verify it in your
~/airflow/logs director tree.  Just go to the proper dag_id and find the latest task log entry and text may
verify something on the lines of:
"""
  File "/home/christopher/airflow_workspace/airflowenv/lib/python3.11/site-packages/airflow/models/xcom.py", line 244, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/christopher/airflow_workspace/airflowenv/lib/python3.11/site-packages/airflow/models/xcom.py", line 659, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/christopher/airflow_workspace/airflowenv/lib/python3.11/site-packages/airflow/utils/json.py", line 104, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/christopher/airflow_workspace/airflowenv/lib/python3.11/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/christopher/airflow_workspace/airflowenv/lib/python3.11/site-packages/airflow/serialization/serde.py", line 143, in serialize
    data, classname, version, is_serialized = _serializers[qn].serialize(o)
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/christopher/airflow_workspace/airflowenv/lib/python3.11/site-packages/airflow/serialization/serializers/pandas.py", line 40, in serialize
    import pyarrow as pa
ModuleNotFoundError: No module named 'pyarrow'
"""
Easily fix this by installing pyarrow module.
20.  Terminal:  pip3 install pyarrow.

Congratulations, you've not only successfully made script work, but you have a means to troubleshoot failing ETL scripts!
Also be on the look out without fault exception handling utf-8 errors, for example with csv data sample sets!


